# RAG란 무엇인가

## 1. 개념

- **RAG = Retrieval-Augmented Generation = 검색 증강 생성**
- **대규모 언어 모델(LLM)이 학습 데이터 외에 추가 정보를 검색하여 답변에 포함시키는 방법을 의미합니다.**

## 2. 등장배경

### LLM의 한계

- 기존의 LLM은 학습시점을 기준으로 답변을 생성해왔습니다. 그렇기에 최신성이 필요한 질문에 대해서는 잘못된 답변을 그럴듯하게 생성해내는 문제가 발생했습니다. 이를 **할루시네이션**이라고도 합니다.
- 또한 답변의 출처가 불분명했기에 전문지식을 필요로 하는 분야의 경우에는 답변의 신뢰도에 의문을 가지게 되었습니다.

### RAG를 통한 한계 보완

- RAG를 통해서 이러한 한계를 보완할 수 있습니다.
- 미리 구현한 내부 데이터를 기반으로 정확도 높은 답변을 생성할 수 있으며, 웹 등 외부 데이터를 검색해서 교차검증을 할 수가 있습니다.

## 3. 현재 AI 환경과 RAG의 필요성

### 최신 AI 시스템의 발전

- 최근 일부 AI 시스템(Claude, ChatGPT의 특정 버전, Perplexity 등)은 실시간 웹 검색 기능과 추론 과정 공개 기능을 제공하고 있습니다.
- 하지만 이런 기능이 없는 기본적인 LLM들(GPT-3.5, 로컬에서 실행되는 오픈소스 모델들)의 경우 여전히 학습 데이터에만 의존합니다.

### RAG가 여전히 필요한 이유

- **전용 데이터**: 회사 내부 문서, 특정 도메인 지식 등 공개되지 않은 데이터 활용
- **제어 가능성**: 검색 범위와 소스를 직접 제어
- **비용 효율성**: 특정 용도에 최적화된 경량 시스템 구축
- **프라이버시**: 외부 서비스에 의존하지 않는 자체 시스템 구현

## 4. 왜 LangChain에서 구현하는가

- LangChain을 사용하여 RAG를 구현한다면 보다 자신의 분야에 커스터마이징된 AI 에이전트를 구현할 수 있습니다.
- LangSmith를 통해서 답변을 추론하기까지의 과정을 자세히 파악할 수 있습니다.
- 다양한 데이터 소스와 벡터 데이터베이스를 통합하여 관리할 수 있습니다.

## 5. RAG 프로세스

### 기존 LLM 사용 방식

**질문 (프롬프트) → LLM → 답변 생성**

### RAG 프로세스

1. **질문 입력**
2. **데이터 전처리** (사전 작업)
    - 문서를 적절한 크기의 청크(chunk)로 분할
    - 각 청크를 벡터(임베딩)로 변환하여 벡터 데이터베이스에 저장
3. **리트리버(Retriever)**
    - 질문을 벡터로 변환(임베딩)
    - 벡터 데이터베이스에서 유사도 계산을 통해 가장 관련성 높은 문서들을 검색
4. **프롬프트 구성**
    - 검색된 문서의 정보를 프롬프트에 통합
    - LLM이 특정 컨텍스트 내에서 작동하도록 역할 부여
5. **LLM 답변 생성**
    - 구성된 프롬프트를 바탕으로 신뢰도 높은 답변 생성
    - 검색된 문서를 근거로 한 정확한 정보 제공
6. **답변 반환 및 출처 제공**
    - 최종 답변과 함께 참조한 문서 출처 제공
    

## 6. RAG의 핵심 기술 구성요소

- **임베딩 모델**: 텍스트를 벡터로 변환하는 모델
- **벡터 데이터베이스**: 임베딩된 문서들을 저장하고 유사도 검색을 수행
- **청킹 전략**: 문서를 적절한 크기로 분할하는 방법
- **프롬프트 엔지니어링**: 검색 결과를 효과적으로 활용하는 프롬프트 설계

## 7. RAG의 한계 및 고려사항

- **검색 품질 의존성**: 검색된 문서의 질이 답변 품질에 직접적 영향
- **계산 비용**: 벡터 검색과 LLM 추론을 모두 수행하므로 비용 증가
- **지연 시간**: 검색 과정으로 인한 응답 시간 증가
- **청킹 전략**: 문서 분할 방식에 따라 정보 손실 가능성