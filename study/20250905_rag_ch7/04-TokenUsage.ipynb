{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95c71f2",
   "metadata": {},
   "source": [
    "# LangChain í† í° ì‚¬ìš©ëŸ‰ í™•ì¸ ê°€ì´ë“œ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” LangChainê³¼ Gemini ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ í† í° ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•˜ê³  ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. í™˜ê²½ ì„¤ì • ë° LangSmith ì—°ê²°\n",
    "2. ê¸°ë³¸ í† í° ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "3. `get_num_tokens()` ë©”ì„œë“œ ì‚¬ìš©ë²•\n",
    "4. ì½œë°±ì„ í†µí•œ í† í° ì¶”ì \n",
    "5. LangSmithë¥¼ í†µí•œ í† í° ëª¨ë‹ˆí„°ë§\n",
    "6. ë°°ì¹˜ ì²˜ë¦¬ ì‹œ í† í° ì‚¬ìš©ëŸ‰\n",
    "7. í† í° ì‚¬ìš©ëŸ‰ ìµœì í™” íŒ\n",
    "\n",
    "## í† í°ì´ë€?\n",
    "\n",
    "**í† í°(Token)**ì€ AI ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„ì…ë‹ˆë‹¤:\n",
    "- **ì˜ì–´**: ë³´í†µ 1ê°œ ë‹¨ì–´ = 1~2ê°œ í† í°\n",
    "- **í•œêµ­ì–´**: ë³´í†µ 1ê°œ ìŒì ˆ = 1ê°œ í† í°\n",
    "- **íŠ¹ìˆ˜ë¬¸ìì™€ ê³µë°±**ë„ í† í°ìœ¼ë¡œ ê³„ì‚°\n",
    "\n",
    "## í† í° ê´€ë¦¬ì˜ ì¤‘ìš”ì„±\n",
    "\n",
    "1. **ë¹„ìš© ìµœì í™”**: ëŒ€ë¶€ë¶„ì˜ AI APIëŠ” í† í° ìˆ˜ì— ë”°ë¼ ìš”ê¸ˆ ë¶€ê³¼\n",
    "2. **ì„±ëŠ¥ í–¥ìƒ**: í† í° ìˆ˜ê°€ ì ì„ìˆ˜ë¡ ì‘ë‹µ ì†ë„ í–¥ìƒ\n",
    "3. **ëª¨ë¸ ì œí•œ**: ê° ëª¨ë¸ë§ˆë‹¤ ìµœëŒ€ í† í° ì œí•œì´ ìˆìŒ\n",
    "4. **ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**: í† í° ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ì—¬ íš¨ìœ¨ì ì¸ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3886a",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° LangSmith ì—°ê²°\n",
    "\n",
    "ë¨¼ì € í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„í¬íŠ¸í•˜ê³  LangSmithë¥¼ ì—°ê²°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01131a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "import time\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "print(\"ğŸ” í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ì¤‘...\")\n",
    "\n",
    "# Google API í‚¤ í™•ì¸\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if google_api_key:\n",
    "    print(\"âœ… Google API Key: ì„¤ì •ë¨\")\n",
    "else:\n",
    "    print(\"âš ï¸  Google API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# OpenAI API í‚¤ í™•ì¸ (ëŒ€ì•ˆ)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key:\n",
    "    print(\"âœ… OpenAI API Key: ì„¤ì •ë¨\")\n",
    "else:\n",
    "    print(\"âš ï¸  OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if not google_api_key and not openai_api_key:\n",
    "    print(\"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   .env íŒŒì¼ì— GOOGLE_API_KEY ë˜ëŠ” OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.\")\n",
    "    print(\"   ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì§„í–‰ë©ë‹ˆë‹¤.\")\n",
    "\n",
    "# LangSmith API í‚¤ í™•ì¸ ë° ì„¤ì •\n",
    "langsmith_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "if langsmith_api_key:\n",
    "    print(\"âœ… LangSmith API Key: ì„¤ì •ë¨\")\n",
    "    # LangSmith ì„¤ì •\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"CH04-Models\"  # í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •\n",
    "    \n",
    "    print(f\"ğŸ“Š LangSmith í”„ë¡œì íŠ¸: {os.environ['LANGCHAIN_PROJECT']}\")\n",
    "    print(f\"ğŸ”„ LangSmith ì¶”ì  í™œì„±í™”: {os.environ['LANGCHAIN_TRACING_V2']}\")\n",
    "    print(\"ğŸŒ LangSmith ëŒ€ì‹œë³´ë“œ: https://smith.langchain.com/\")\n",
    "else:\n",
    "    print(\"âš ï¸  LangSmith API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   í† í° ì¶”ì ì€ ì½œë°±ì„ í†µí•´ì„œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\nâœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™” - ìš°ì„ ìˆœìœ„: OpenAI > Gemini > ì‹œë®¬ë ˆì´ì…˜\n",
    "llm = None\n",
    "model_type = \"simulation\"\n",
    "\n",
    "# 1. OpenAI ëª¨ë¸ ì‹œë„ (ì¶”ì²œ)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key:\n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        print(\"ğŸ¤– OpenAI GPT-3.5-turbo ëª¨ë¸ ì‚¬ìš©!\")\n",
    "        model_type = \"openai\"\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
    "        print(\"ğŸ§ª ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸...\")\n",
    "        test_response = llm.invoke(\"Hi\")\n",
    "        print(f\"âœ… OpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  OpenAI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "        llm = None\n",
    "\n",
    "# 2. Gemini ëª¨ë¸ ì‹œë„ (OpenAIê°€ ì‹¤íŒ¨í•œ ê²½ìš°)\n",
    "if llm is None:\n",
    "    google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if google_api_key:\n",
    "        try:\n",
    "            llm = ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-1.5-flash\",\n",
    "                temperature=0.7,\n",
    "                max_output_tokens=200\n",
    "            )\n",
    "            print(\"ğŸ¤– Gemini ëª¨ë¸ ì‚¬ìš©!\")\n",
    "            model_type = \"gemini\"\n",
    "            \n",
    "            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ê±´ë„ˆë›°ê¸°)\n",
    "            print(\"ğŸ§ª ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸...\")\n",
    "            test_response = llm.invoke(\"Hi\")\n",
    "            print(f\"âœ… Gemini ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Gemini ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€: {str(e)[:100]}...\")\n",
    "            if \"quota\" in str(e).lower() or \"429\" in str(e):\n",
    "                print(\"   â†’ API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ì¸í•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤.\")\n",
    "            llm = None\n",
    "\n",
    "# 3. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ëª¨ë“  APIê°€ ì‹¤íŒ¨í•œ ê²½ìš°)\n",
    "if llm is None:\n",
    "    print(\"ğŸ”§ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜!\")\n",
    "    print(\"   â†’ ì‹¤ì œ LLM í˜¸ì¶œ ì—†ì´ í† í° ê³„ì‚° ë° ë°ëª¨ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    model_type = \"simulation\"\n",
    "    \n",
    "    # ì‹œë®¬ë ˆì´ì…˜ìš© ê°€ì§œ LLM í´ë˜ìŠ¤\n",
    "    class SimulationLLM:\n",
    "        def __init__(self):\n",
    "            self.model = \"simulation-model\"\n",
    "            self.temperature = 0.7\n",
    "            \n",
    "        def get_num_tokens(self, text):\n",
    "            # ëŒ€ëµì ì¸ í† í° ê³„ì‚° (ì˜ì–´: 4ë¬¸ìë‹¹ 1í† í°, í•œêµ­ì–´: 1ë¬¸ìë‹¹ 1í† í°)\n",
    "            korean_chars = len([c for c in text if ord(c) > 127])\n",
    "            english_chars = len(text) - korean_chars\n",
    "            return korean_chars + (english_chars // 4)\n",
    "        \n",
    "        def invoke(self, messages):\n",
    "            # ì‹œë®¬ë ˆì´ì…˜ ì‘ë‹µ\n",
    "            class SimResponse:\n",
    "                def __init__(self):\n",
    "                    self.content = \"ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì‘ë‹µì…ë‹ˆë‹¤. ì‹¤ì œ LLMì€ í˜¸ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\n",
    "            return SimResponse()\n",
    "    \n",
    "    llm = SimulationLLM()\n",
    "\n",
    "# ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "print(f\"\\nğŸ“‹ ìµœì¢… ëª¨ë¸ ì„¤ì •:\")\n",
    "print(f\"   ğŸ¤– ëª¨ë¸: {llm.model if hasattr(llm, 'model') else 'Unknown'}\")\n",
    "print(f\"   ğŸŒ¡ï¸  ì˜¨ë„: {llm.temperature if hasattr(llm, 'temperature') else 'Unknown'}\")\n",
    "print(f\"   ğŸ”§ ëª¨ë“œ: {model_type}\")\n",
    "\n",
    "if model_type == \"simulation\":\n",
    "    print(f\"\\nğŸ’¡ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì•ˆë‚´:\")\n",
    "    print(f\"   â€¢ í† í° ê³„ì‚° ê¸°ëŠ¥ì€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤\")\n",
    "    print(f\"   â€¢ LLM í˜¸ì¶œì´ í•„ìš”í•œ ì…€ì€ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\")\n",
    "    print(f\"   â€¢ ì‹¤ì œ API ë¹„ìš©ì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\")\n",
    "\n",
    "print(f\"\\nâœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8288507",
   "metadata": {},
   "source": [
    "## 2. ê¸°ë³¸ í† í° ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "\n",
    "LangChainì—ì„œ í† í° ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ë“¤ì„ ì•Œì•„ë´…ì‹œë‹¤.\n",
    "\n",
    "### í† í° ê³„ì‚° ë°©ë²•\n",
    "1. **`get_num_tokens()`**: í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ë¯¸ë¦¬ ê³„ì‚°\n",
    "2. **ì‘ë‹µ ë©”íƒ€ë°ì´í„°**: ì‹¤ì œ API í˜¸ì¶œ í›„ ì‚¬ìš©ëœ í† í° í™•ì¸\n",
    "3. **ì½œë°± í•¸ë“¤ëŸ¬**: ì‹¤ì‹œê°„ í† í° ì¶”ì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1681afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# ì‘ë‹µ ë©”íƒ€ë°ì´í„°ë¥¼ í†µí•œ í† í° ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "############################################################################\n",
    "\n",
    "print(\"ğŸ“Š ì‘ë‹µ ë©”íƒ€ë°ì´í„°ë¥¼ í†µí•œ í† í° ì‚¬ìš©ëŸ‰ í™•ì¸\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "test_questions = [\n",
    "    \"ì•ˆë…•í•˜ì„¸ìš”!\",\n",
    "    \"Python í”„ë¡œê·¸ë˜ë°ì˜ ì¥ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ê³ , ê°ê°ì˜ í™œìš© ë¶„ì•¼ì™€ ë¯¸ë˜ ì „ë§ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. ì§ˆë¬¸: {question}\")\n",
    "    print(f\"   ì§ˆë¬¸ ê¸¸ì´: {len(question)} ë¬¸ì\")\n",
    "    \n",
    "    # ë©”ì‹œì§€ ìƒì„±\n",
    "    message = HumanMessage(content=question)\n",
    "    \n",
    "    # API í˜¸ì¶œ\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    # ì‘ë‹µ ì •ë³´\n",
    "    print(f\"   ì‘ë‹µ: {response.content[:50]}...\")\n",
    "    print(f\"   ì‘ë‹µ ê¸¸ì´: {len(response.content)} ë¬¸ì\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„°ì—ì„œ í† í° ì •ë³´ í™•ì¸\n",
    "    if hasattr(response, 'response_metadata'):\n",
    "        metadata = response.response_metadata\n",
    "        print(f\"   ğŸ“ˆ ë©”íƒ€ë°ì´í„°: {metadata}\")\n",
    "        \n",
    "        # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ\n",
    "        if 'usage_metadata' in metadata:\n",
    "            usage = metadata['usage_metadata']\n",
    "            print(f\"   ğŸ¯ ì…ë ¥ í† í°: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "            print(f\"   ğŸ¯ ì¶œë ¥ í† í°: {usage.get('completion_tokens', 'N/A')}\")\n",
    "            print(f\"   ğŸ¯ ì´ í† í°: {usage.get('total_tokens', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  í† í° ì •ë³´ê°€ ë©”íƒ€ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  ì‘ë‹µì— ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\nâœ… ë©”íƒ€ë°ì´í„° í† í° í™•ì¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec626e",
   "metadata": {},
   "source": [
    "## 3. `get_num_tokens()` ë©”ì„œë“œ ì‚¬ìš©ë²•\n",
    "\n",
    "`get_num_tokens()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ì‹¤ì œ APIë¥¼ í˜¸ì¶œí•˜ê¸° ì „ì— í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” í™œìš© ì‚¬ë¡€\n",
    "- **ë¹„ìš© ì˜ˆì¸¡**: API í˜¸ì¶œ ì „ ë¹„ìš© ê³„ì‚°\n",
    "- **í† í° ì œí•œ í™•ì¸**: ëª¨ë¸ì˜ ìµœëŒ€ í† í° ì œí•œ ì¤€ìˆ˜\n",
    "- **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: í† í° ìˆ˜ë¥¼ ê³ ë ¤í•œ í”„ë¡¬í”„íŠ¸ ì„¤ê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# get_num_tokens() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œ í† í° ìˆ˜ ê³„ì‚°\n",
    "############################################################################\n",
    "\n",
    "print(\"ğŸ”¢ get_num_tokens() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œ í† í° ê³„ì‚°\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸\n",
    "test_texts = [\n",
    "    \"ì•ˆë…•í•˜ì„¸ìš”!\",\n",
    "    \"Hello, world!\",\n",
    "    \"Pythonì€ ë°°ìš°ê¸° ì‰½ê³  ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì œê³µí•˜ì—¬ ë³µì¡í•œ AI ì›Œí¬í”Œë¡œìš°ë¥¼ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"\"\"\n",
    "    ê¸´ í…ìŠ¤íŠ¸ ì˜ˆì œì…ë‹ˆë‹¤. \n",
    "    ì—¬ëŸ¬ ì¤„ì— ê±¸ì¹œ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•´ë´…ì‹œë‹¤.\n",
    "    í† í° ê³„ì‚°ì€ ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë©°,\n",
    "    í•œêµ­ì–´ì™€ ì˜ì–´ì˜ í† í° ê³„ì‚° ë°©ì‹ë„ ë‹¤ë¦…ë‹ˆë‹¤.\n",
    "    ì´ëŸ° ì°¨ì´ì ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ í…ìŠ¤íŠ¸ë³„ í† í° ìˆ˜ ë¶„ì„:\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    # í† í° ìˆ˜ ê³„ì‚°\n",
    "    try:\n",
    "        token_count = llm.get_num_tokens(text)\n",
    "        char_count = len(text)\n",
    "        \n",
    "        print(f\"\\n{i}. í…ìŠ¤íŠ¸: {text[:50]}{'...' if len(text) > 50 else ''}\")\n",
    "        print(f\"   ğŸ“ ë¬¸ì ìˆ˜: {char_count}\")\n",
    "        print(f\"   ğŸ¯ í† í° ìˆ˜: {token_count}\")\n",
    "        print(f\"   ğŸ“Š í† í°/ë¬¸ì ë¹„ìœ¨: {token_count/char_count:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{i}. í…ìŠ¤íŠ¸: {text[:50]}{'...' if len(text) > 50 else ''}\")\n",
    "        print(f\"   âŒ í† í° ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ë©”ì‹œì§€ ê°ì²´ì˜ í† í° ìˆ˜ ê³„ì‚°\n",
    "print(f\"\\nğŸ” ë©”ì‹œì§€ ê°ì²´ì˜ í† í° ìˆ˜ ê³„ì‚°:\")\n",
    "messages = [\n",
    "    SystemMessage(content=\"ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"),\n",
    "    HumanMessage(content=\"Python í”„ë¡œê·¸ë˜ë°ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\n",
    "]\n",
    "\n",
    "try:\n",
    "    total_tokens = llm.get_num_tokens_from_messages(messages)\n",
    "    print(f\"   ğŸ“¨ ì „ì²´ ë©”ì‹œì§€ í† í° ìˆ˜: {total_tokens}\")\n",
    "    \n",
    "    for i, msg in enumerate(messages):\n",
    "        individual_tokens = llm.get_num_tokens(msg.content)\n",
    "        print(f\"   ğŸ“© ë©”ì‹œì§€ {i+1} ({type(msg).__name__}): {individual_tokens} í† í°\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ ë©”ì‹œì§€ í† í° ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "    # ê°œë³„ ë©”ì‹œì§€ë¡œ ê³„ì‚° ì‹œë„\n",
    "    total = 0\n",
    "    for i, msg in enumerate(messages):\n",
    "        try:\n",
    "            tokens = llm.get_num_tokens(msg.content)\n",
    "            total += tokens\n",
    "            print(f\"   ğŸ“© ë©”ì‹œì§€ {i+1} ({type(msg).__name__}): {tokens} í† í°\")\n",
    "        except Exception as e2:\n",
    "            print(f\"   âŒ ë©”ì‹œì§€ {i+1} ê³„ì‚° ì‹¤íŒ¨: {e2}\")\n",
    "    print(f\"   ğŸ“¨ ê°œë³„ ê³„ì‚° ì´í•©: {total} í† í°\")\n",
    "\n",
    "print(\"\\nâœ… get_num_tokens() ì‹¤ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779fe839",
   "metadata": {},
   "source": [
    "## 4. ì½œë°±ì„ í†µí•œ í† í° ì¶”ì \n",
    "\n",
    "ì½œë°± í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ í† í° ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•˜ê³  ë¡œê¹…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ì½œë°±ì˜ ì¥ì \n",
    "- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: API í˜¸ì¶œ ì¤‘ í† í° ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ í™•ì¸\n",
    "- **ì„¸ë¶€ ì¶”ì **: ê° ë‹¨ê³„ë³„ í† í° ì‚¬ìš©ëŸ‰ ìƒì„¸ ë¶„ì„\n",
    "- **ì»¤ìŠ¤í…€ ë¡œê¹…**: ì›í•˜ëŠ” í˜•íƒœë¡œ í† í° ì •ë³´ ê¸°ë¡\n",
    "- **ë¹„ìš© ê´€ë¦¬**: ì‹¤ì‹œê°„ ë¹„ìš© ê³„ì‚° ë° ì•Œë¦¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ee0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# í† í° ì¶”ì ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬ êµ¬í˜„\n",
    "############################################################################\n",
    "\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from typing import Any, Dict, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "class TokenUsageCallback(BaseCallbackHandler):\n",
    "    \"\"\"í† í° ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.total_prompt_tokens = 0\n",
    "        self.total_completion_tokens = 0\n",
    "        self.call_count = 0\n",
    "        self.cost_per_1k_tokens = 0.001  # Gemini Flash ì˜ˆìƒ ë¹„ìš© (USD)\n",
    "        self.calls_history = []\n",
    "    \n",
    "    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:\n",
    "        \"\"\"LLM í˜¸ì¶œ ì‹œì‘ ì‹œ ì‹¤í–‰\"\"\"\n",
    "        self.call_count += 1\n",
    "        print(f\"\\nğŸš€ LLM í˜¸ì¶œ #{self.call_count} ì‹œì‘\")\n",
    "        print(f\"   ğŸ“ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(prompts)}\")\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ì˜ ì˜ˆìƒ í† í° ìˆ˜ ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"   ğŸ“„ í”„ë¡¬í”„íŠ¸ {i+1}: {prompt[:50]}...\")\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        \"\"\"LLM í˜¸ì¶œ ì™„ë£Œ ì‹œ ì‹¤í–‰\"\"\"\n",
    "        print(f\"âœ… LLM í˜¸ì¶œ #{self.call_count} ì™„ë£Œ\")\n",
    "        \n",
    "        # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ\n",
    "        if response.llm_output:\n",
    "            usage = response.llm_output.get('usage_metadata', {})\n",
    "            \n",
    "            prompt_tokens = usage.get('prompt_tokens', 0)\n",
    "            completion_tokens = usage.get('completion_tokens', 0)\n",
    "            total_tokens_call = usage.get('total_tokens', 0)\n",
    "            \n",
    "            # ëˆ„ì  í†µê³„ ì—…ë°ì´íŠ¸\n",
    "            self.total_prompt_tokens += prompt_tokens\n",
    "            self.total_completion_tokens += completion_tokens\n",
    "            self.total_tokens += total_tokens_call\n",
    "            \n",
    "            # ì´ë²ˆ í˜¸ì¶œ ì •ë³´ ì €ì¥\n",
    "            call_info = {\n",
    "                'call_number': self.call_count,\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'total_tokens': total_tokens_call,\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "            self.calls_history.append(call_info)\n",
    "            \n",
    "            # ìƒì„¸ ì •ë³´ ì¶œë ¥\n",
    "            print(f\"   ğŸ¯ ì´ë²ˆ í˜¸ì¶œ í† í°:\")\n",
    "            print(f\"      ì…ë ¥: {prompt_tokens} í† í°\")\n",
    "            print(f\"      ì¶œë ¥: {completion_tokens} í† í°\")\n",
    "            print(f\"      ì´í•©: {total_tokens_call} í† í°\")\n",
    "            \n",
    "            # ì˜ˆìƒ ë¹„ìš© ê³„ì‚°\n",
    "            estimated_cost = (total_tokens_call / 1000) * self.cost_per_1k_tokens\n",
    "            print(f\"   ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.6f}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   âš ï¸  í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    def on_llm_error(self, error: Exception, **kwargs) -> None:\n",
    "        \"\"\"LLM í˜¸ì¶œ ì—ëŸ¬ ì‹œ ì‹¤í–‰\"\"\"\n",
    "        print(f\"âŒ LLM í˜¸ì¶œ #{self.call_count} ì—ëŸ¬: {error}\")\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ì •ë³´ ë°˜í™˜\"\"\"\n",
    "        total_cost = (self.total_tokens / 1000) * self.cost_per_1k_tokens\n",
    "        \n",
    "        summary = {\n",
    "            'total_calls': self.call_count,\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'total_prompt_tokens': self.total_prompt_tokens,\n",
    "            'total_completion_tokens': self.total_completion_tokens,\n",
    "            'estimated_total_cost': total_cost,\n",
    "            'average_tokens_per_call': self.total_tokens / max(self.call_count, 1),\n",
    "            'calls_history': self.calls_history\n",
    "        }\n",
    "        return summary\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ì„ ì˜ˆì˜ê²Œ ì¶œë ¥\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½\")\n",
    "        print(f\"=\" * 40)\n",
    "        print(f\"ğŸ”¢ ì´ í˜¸ì¶œ ìˆ˜: {summary['total_calls']}\")\n",
    "        print(f\"ğŸ¯ ì´ í† í° ìˆ˜: {summary['total_tokens']:,}\")\n",
    "        print(f\"   â”œâ”€ ì…ë ¥ í† í°: {summary['total_prompt_tokens']:,}\")\n",
    "        print(f\"   â””â”€ ì¶œë ¥ í† í°: {summary['total_completion_tokens']:,}\")\n",
    "        print(f\"ğŸ“Š í˜¸ì¶œë‹¹ í‰ê· : {summary['average_tokens_per_call']:.1f} í† í°\")\n",
    "        print(f\"ğŸ’° ì˜ˆìƒ ì´ ë¹„ìš©: ${summary['estimated_total_cost']:.6f}\")\n",
    "\n",
    "# ì½œë°± í•¸ë“¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "token_callback = TokenUsageCallback()\n",
    "print(\"âœ… í† í° ì¶”ì  ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# ì½œë°± í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•œ í† í° ì¶”ì  í…ŒìŠ¤íŠ¸\n",
    "############################################################################\n",
    "\n",
    "print(\"ğŸ§ª ì½œë°± í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•œ í† í° ì¶”ì  í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤\n",
    "test_questions = [\n",
    "    \"ì•ˆë…•í•˜ì„¸ìš”!\",\n",
    "    \"Pythonì˜ ì£¼ìš” íŠ¹ì§• 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•˜ê³ , ê°ê°ì˜ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "]\n",
    "\n",
    "# ê° ì§ˆë¬¸ì— ëŒ€í•´ ì½œë°±ê³¼ í•¨ê»˜ LLM í˜¸ì¶œ\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*20} ì§ˆë¬¸ {i} {'='*20}\")\n",
    "    print(f\"â“ ì§ˆë¬¸: {question}\")\n",
    "    \n",
    "    # ì½œë°±ì„ í¬í•¨í•˜ì—¬ LLM í˜¸ì¶œ\n",
    "    response = llm.invoke(\n",
    "        [HumanMessage(content=question)],\n",
    "        config={\"callbacks\": [token_callback]}\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ’¬ ì‘ë‹µ: {response.content}\")\n",
    "    print(f\"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response.content)} ë¬¸ì\")\n",
    "\n",
    "# ì „ì²´ í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ì¶œë ¥\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "token_callback.print_summary()\n",
    "\n",
    "# ì„¸ë¶€ í˜¸ì¶œ ì´ë ¥ ì¶œë ¥\n",
    "print(f\"\\nğŸ“‹ ì„¸ë¶€ í˜¸ì¶œ ì´ë ¥:\")\n",
    "for call in token_callback.calls_history:\n",
    "    print(f\"   í˜¸ì¶œ #{call['call_number']}: \"\n",
    "          f\"ì…ë ¥ {call['prompt_tokens']} + ì¶œë ¥ {call['completion_tokens']} \"\n",
    "          f\"= ì´ {call['total_tokens']} í† í°\")\n",
    "\n",
    "print(\"\\nâœ… ì½œë°± ê¸°ë°˜ í† í° ì¶”ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7961619",
   "metadata": {},
   "source": [
    "## 5. LangSmithë¥¼ í†µí•œ í† í° ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "LangSmithëŠ” LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ëª¨ë‹ˆí„°ë§ê³¼ ë””ë²„ê¹…ì„ ìœ„í•œ í”Œë«í¼ì…ë‹ˆë‹¤. í† í° ì‚¬ìš©ëŸ‰ë„ ìë™ìœ¼ë¡œ ì¶”ì ë©ë‹ˆë‹¤.\n",
    "\n",
    "### LangSmithì˜ í† í° ì¶”ì  ê¸°ëŠ¥\n",
    "- **ìë™ ìˆ˜ì§‘**: ëª¨ë“  LLM í˜¸ì¶œì˜ í† í° ì‚¬ìš©ëŸ‰ ìë™ ê¸°ë¡\n",
    "- **ì‹œê°í™”**: ëŒ€ì‹œë³´ë“œë¥¼ í†µí•œ í† í° ì‚¬ìš©ëŸ‰ ì‹œê°í™”\n",
    "- **ë¹„ìš© ë¶„ì„**: í”„ë¡œì íŠ¸ë³„, ì‹œê°„ë³„ ë¹„ìš© ë¶„ì„\n",
    "- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: í† í° íš¨ìœ¨ì„± ë° ìµœì í™” ì§€ì  íŒŒì•…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae35b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# LangSmithë¥¼ í†µí•œ í† í° ì¶”ì  í…ŒìŠ¤íŠ¸\n",
    "############################################################################\n",
    "\n",
    "print(\"ğŸŒ LangSmithë¥¼ í†µí•œ í† í° ì¶”ì  í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# LangSmith ì¶”ì  ìƒíƒœ í™•ì¸\n",
    "langsmith_enabled = os.getenv(\"LANGCHAIN_TRACING_V2\") == \"true\"\n",
    "project_name = os.getenv(\"LANGCHAIN_PROJECT\", \"ê¸°ë³¸ í”„ë¡œì íŠ¸\")\n",
    "\n",
    "print(f\"ğŸ“Š LangSmith ì¶”ì : {'âœ… í™œì„±í™”' if langsmith_enabled else 'âŒ ë¹„í™œì„±í™”'}\")\n",
    "print(f\"ğŸ“ í”„ë¡œì íŠ¸ëª…: {project_name}\")\n",
    "\n",
    "if langsmith_enabled:\n",
    "    print(f\"ğŸŒ ëŒ€ì‹œë³´ë“œ: https://smith.langchain.com/\")\n",
    "    print(f\"   â†’ í”„ë¡œì íŠ¸ '{project_name}'ì—ì„œ ì‹¤ì‹œê°„ í† í° ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # LangSmith ì¶”ì ê³¼ í•¨ê»˜ ì²´ì¸ ì‹¤í–‰\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    \n",
    "    print(f\"\\nğŸ”— ì²´ì¸ì„ í†µí•œ LangSmith ì¶”ì  í…ŒìŠ¤íŠ¸\")\n",
    "    \n",
    "    # ê°„ë‹¨í•œ ì²´ì¸ ìƒì„±\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”: {topic}\"\n",
    "    )\n",
    "    parser = StrOutputParser()\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    # ì—¬ëŸ¬ ì£¼ì œë¡œ ì²´ì¸ ì‹¤í–‰\n",
    "    topics = [\"ì¸ê³µì§€ëŠ¥\", \"ë¸”ë¡ì²´ì¸\", \"ì–‘ìì»´í“¨íŒ…\"]\n",
    "    \n",
    "    for topic in topics:\n",
    "        print(f\"\\nğŸ¯ ì£¼ì œ: {topic}\")\n",
    "        result = chain.invoke({\"topic\": topic})\n",
    "        print(f\"ğŸ“ ì‘ë‹µ: {result[:100]}...\")\n",
    "        print(f\"âœ… LangSmithì— í† í° ì‚¬ìš©ëŸ‰ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    print(f\"   â€¢ ê° í˜¸ì¶œì˜ í† í° ì‚¬ìš©ëŸ‰\")\n",
    "    print(f\"   â€¢ ì…ë ¥/ì¶œë ¥ í† í° ë¶„ì„\")\n",
    "    print(f\"   â€¢ ì‹œê°„ë³„ ì‚¬ìš©ëŸ‰ ì¶”ì´\")\n",
    "    print(f\"   â€¢ ë¹„ìš© ë¶„ì„ ë° ì˜ˆì¸¡\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nâš ï¸  LangSmithê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   í™˜ê²½ ë³€ìˆ˜ LANGCHAIN_API_KEYë¥¼ ì„¤ì •í•˜ê³  LANGCHAIN_TRACING_V2=trueë¡œ ì„¤ì •í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # LangSmith ì—†ì´ë„ ê¸°ë³¸ ì¶”ì  ì‹œì—°\n",
    "    print(f\"\\nğŸ”§ ë¡œì»¬ í† í° ì¶”ì ìœ¼ë¡œ ëŒ€ì²´ ì‹¤í–‰:\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”: {topic}\"\n",
    "    )\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    # ìƒˆë¡œìš´ ì½œë°± ì¸ìŠ¤í„´ìŠ¤ë¡œ ì²´ì¸ í…ŒìŠ¤íŠ¸\n",
    "    local_callback = TokenUsageCallback()\n",
    "    \n",
    "    topics = [\"ì¸ê³µì§€ëŠ¥\", \"ë¸”ë¡ì²´ì¸\"]\n",
    "    for topic in topics:\n",
    "        print(f\"\\nğŸ¯ ì£¼ì œ: {topic}\")\n",
    "        \n",
    "        # ê° ë‹¨ê³„ë³„ë¡œ í† í° ì¶”ì \n",
    "        formatted_prompt = prompt.format_messages(topic=topic)\n",
    "        response = llm.invoke(\n",
    "            formatted_prompt, \n",
    "            config={\"callbacks\": [local_callback]}\n",
    "        )\n",
    "        result = parser.invoke(response)\n",
    "        \n",
    "        print(f\"ğŸ“ ì‘ë‹µ: {result[:100]}...\")\n",
    "    \n",
    "    # ë¡œì»¬ ì¶”ì  ê²°ê³¼ ìš”ì•½\n",
    "    local_callback.print_summary()\n",
    "\n",
    "print(f\"\\nâœ… LangSmith í† í° ì¶”ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46b0ae",
   "metadata": {},
   "source": [
    "## 6. ë°°ì¹˜ ì²˜ë¦¬ ì‹œ í† í° ì‚¬ìš©ëŸ‰\n",
    "\n",
    "ì—¬ëŸ¬ ê°œì˜ ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•  ë•Œ í† í° ì‚¬ìš©ëŸ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ì‹œë‹¤.\n",
    "\n",
    "### ë°°ì¹˜ ì²˜ë¦¬ì˜ ì¥ì \n",
    "- **ë¹„ìš© íš¨ìœ¨ì„±**: ê°œë³„ í˜¸ì¶œë³´ë‹¤ ë°°ì¹˜ ì²˜ë¦¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ ë” íš¨ìœ¨ì \n",
    "- **ì²˜ë¦¬ ì†ë„**: ë™ì‹œ ì²˜ë¦¬ë¡œ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•\n",
    "- **í† í° ì˜ˆì¸¡**: ì „ì²´ ë°°ì¹˜ì˜ í† í° ì‚¬ìš©ëŸ‰ ë¯¸ë¦¬ ê³„ì‚° ê°€ëŠ¥\n",
    "- **ì—ëŸ¬ ì²˜ë¦¬**: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—ëŸ¬ ê´€ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4368589",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# ë°°ì¹˜ ì²˜ë¦¬ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ê´€ë¦¬\n",
    "############################################################################\n",
    "\n",
    "print(\"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ê´€ë¦¬\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ë°°ì¹˜ ì²˜ë¦¬ìš© ì§ˆë¬¸ ëª©ë¡\n",
    "batch_questions = [\n",
    "    \"Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "    \"ë”¥ëŸ¬ë‹ê³¼ ì‹ ê²½ë§ì˜ ê´€ê³„ëŠ”?\",\n",
    "    \"ìì—°ì–´ ì²˜ë¦¬ì˜ ì£¼ìš” ê¸°ìˆ ë“¤ì€?\",\n",
    "    \"LangChainì˜ í•µì‹¬ ê¸°ëŠ¥ì€?\",\n",
    "    \"AI ìœ¤ë¦¬ì˜ ì¤‘ìš”ì„±ì€?\",\n",
    "    \"í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì˜ ì¥ì ì€?\",\n",
    "    \"ë°ì´í„° ê³¼í•™ìì˜ ì—­í• ì€?\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“‹ ë°°ì¹˜ í¬ê¸°: {len(batch_questions)}ê°œ ì§ˆë¬¸\")\n",
    "\n",
    "# 1. ì‚¬ì „ í† í° ê³„ì‚°\n",
    "print(f\"\\n1ï¸âƒ£ ì‚¬ì „ í† í° ê³„ì‚°\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "total_estimated_tokens = 0\n",
    "question_tokens = []\n",
    "\n",
    "for i, question in enumerate(batch_questions, 1):\n",
    "    try:\n",
    "        tokens = llm.get_num_tokens(question)\n",
    "        question_tokens.append(tokens)\n",
    "        total_estimated_tokens += tokens\n",
    "        print(f\"   ì§ˆë¬¸ {i}: {tokens} í† í° - {question}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ì§ˆë¬¸ {i}: í† í° ê³„ì‚° ì‹¤íŒ¨ - {question}\")\n",
    "        question_tokens.append(0)\n",
    "\n",
    "print(f\"\\nğŸ“Š ì‚¬ì „ ê³„ì‚° ê²°ê³¼:\")\n",
    "print(f\"   ì´ ì…ë ¥ í† í° (ì˜ˆìƒ): {total_estimated_tokens}\")\n",
    "print(f\"   í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {total_estimated_tokens/len(batch_questions):.1f} í† í°\")\n",
    "\n",
    "# ì¶œë ¥ í† í° ì˜ˆìƒ (í‰ê· ì ìœ¼ë¡œ ì…ë ¥ì˜ 2-3ë°° ê°€ì •)\n",
    "estimated_output_tokens = total_estimated_tokens * 2.5\n",
    "estimated_total = total_estimated_tokens + estimated_output_tokens\n",
    "estimated_cost = (estimated_total / 1000) * 0.001\n",
    "\n",
    "print(f\"   ì˜ˆìƒ ì¶œë ¥ í† í°: {estimated_output_tokens:.0f}\")\n",
    "print(f\"   ì˜ˆìƒ ì´ í† í°: {estimated_total:.0f}\")\n",
    "print(f\"   ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.6f}\")\n",
    "\n",
    "# 2. ë°°ì¹˜ ì‹¤í–‰ (ì½œë°±ê³¼ í•¨ê»˜)\n",
    "print(f\"\\n2ï¸âƒ£ ë°°ì¹˜ ì‹¤í–‰ (í† í° ì¶”ì )\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "batch_callback = TokenUsageCallback()\n",
    "batch_responses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, question in enumerate(batch_questions, 1):\n",
    "    print(f\"\\nâ³ ì²˜ë¦¬ ì¤‘: {i}/{len(batch_questions)} - {question[:30]}...\")\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(\n",
    "            [HumanMessage(content=question)],\n",
    "            config={\"callbacks\": [batch_callback]}\n",
    "        )\n",
    "        batch_responses.append({\n",
    "            'question': question,\n",
    "            'response': response.content,\n",
    "            'success': True\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ì—ëŸ¬: {e}\")\n",
    "        batch_responses.append({\n",
    "            'question': question,\n",
    "            'response': None,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "# 3. ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„\n",
    "print(f\"\\n3ï¸âƒ£ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "successful_responses = [r for r in batch_responses if r['success']]\n",
    "failed_responses = [r for r in batch_responses if not r['success']]\n",
    "\n",
    "print(f\"âœ… ì„±ê³µ: {len(successful_responses)}ê°œ\")\n",
    "print(f\"âŒ ì‹¤íŒ¨: {len(failed_responses)}ê°œ\")\n",
    "print(f\"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\")\n",
    "print(f\"ğŸ“Š í‰ê·  ì²˜ë¦¬ ì‹œê°„: {processing_time/len(batch_questions):.2f}ì´ˆ/ì§ˆë¬¸\")\n",
    "\n",
    "# í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½\n",
    "batch_callback.print_summary()\n",
    "\n",
    "# ì‹¤ì œ vs ì˜ˆìƒ ë¹„êµ\n",
    "actual_total = batch_callback.total_tokens\n",
    "prediction_accuracy = (1 - abs(estimated_total - actual_total) / actual_total) * 100\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ì˜ˆìƒ vs ì‹¤ì œ ë¹„êµ:\")\n",
    "print(f\"   ì˜ˆìƒ ì´ í† í°: {estimated_total:.0f}\")\n",
    "print(f\"   ì‹¤ì œ ì´ í† í°: {actual_total}\")\n",
    "print(f\"   ì˜ˆì¸¡ ì •í™•ë„: {prediction_accuracy:.1f}%\")\n",
    "\n",
    "# 4. ì‘ë‹µ ìƒ˜í”Œ ì¶œë ¥\n",
    "print(f\"\\n4ï¸âƒ£ ì‘ë‹µ ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, response in enumerate(successful_responses[:3], 1):\n",
    "    print(f\"\\nì§ˆë¬¸ {i}: {response['question']}\")\n",
    "    print(f\"ì‘ë‹µ: {response['response'][:100]}...\")\n",
    "\n",
    "print(f\"\\nâœ… ë°°ì¹˜ ì²˜ë¦¬ í† í° ê´€ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167904fc",
   "metadata": {},
   "source": [
    "## 7. í† í° ì‚¬ìš©ëŸ‰ ìµœì í™” íŒ\n",
    "\n",
    "í† í° ì‚¬ìš©ëŸ‰ì„ ìµœì í™”í•˜ì—¬ ë¹„ìš©ì„ ì ˆì•½í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì‹¤ìš©ì ì¸ ë°©ë²•ë“¤ì„ ì•Œì•„ë´…ì‹œë‹¤.\n",
    "\n",
    "### ìµœì í™” ì „ëµ\n",
    "1. **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°, ëª…í™•í•œ ì§€ì‹œ\n",
    "2. **ì‘ë‹µ ê¸¸ì´ ì œí•œ**: `max_output_tokens` ì„¤ì •ìœ¼ë¡œ ì¶œë ¥ ì œí•œ\n",
    "3. **ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™œìš©**: ë°˜ë³µì ì¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ë¶„ë¦¬\n",
    "4. **ìºì‹± í™œìš©**: ë™ì¼í•œ ìš”ì²­ì˜ ì¬ì‚¬ìš©\n",
    "5. **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¬¶ì–´ì„œ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49758071",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# í† í° ì‚¬ìš©ëŸ‰ ìµœì í™” ì‹¤ìŠµ\n",
    "############################################################################\n",
    "\n",
    "print(\"âš¡ í† í° ì‚¬ìš©ëŸ‰ ìµœì í™” ì‹¤ìŠµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. í”„ë¡¬í”„íŠ¸ ìµœì í™” ë¹„êµ\n",
    "print(\"1ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ìµœì í™” ë¹„êµ\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ë¹„íš¨ìœ¨ì ì¸ í”„ë¡¬í”„íŠ¸ vs ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸\n",
    "inefficient_prompt = \"\"\"\n",
    "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í˜„ì¬ Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ëŒ€í•´ì„œ ë°°ìš°ê³  ìˆëŠ” í•™ìƒì…ë‹ˆë‹¤. \n",
    "Pythonì´ë¼ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ê°€ ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë“¤ê³¼ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ ì¥ì ë“¤ì´ ìˆëŠ”ì§€, \n",
    "ê·¸ë¦¬ê³  ì–´ë–¤ ë¶„ì•¼ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ì— ëŒ€í•´ì„œ ìì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. \n",
    "ê°€ëŠ¥í•˜ë©´ ì˜ˆì‹œë„ í•¨ê»˜ ë“¤ì–´ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "efficient_prompt = \"Pythonì˜ ì£¼ìš” ì¥ì  3ê°€ì§€ì™€ í™œìš© ë¶„ì•¼ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# í† í° ìˆ˜ ë¹„êµ\n",
    "inefficient_tokens = llm.get_num_tokens(inefficient_prompt.strip())\n",
    "efficient_tokens = llm.get_num_tokens(efficient_prompt)\n",
    "\n",
    "print(f\"âŒ ë¹„íš¨ìœ¨ì  í”„ë¡¬í”„íŠ¸:\")\n",
    "print(f\"   ë‚´ìš©: {inefficient_prompt.strip()[:50]}...\")\n",
    "print(f\"   í† í° ìˆ˜: {inefficient_tokens}\")\n",
    "\n",
    "print(f\"\\nâœ… ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸:\")\n",
    "print(f\"   ë‚´ìš©: {efficient_prompt}\")\n",
    "print(f\"   í† í° ìˆ˜: {efficient_tokens}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ìµœì í™” ê²°ê³¼:\")\n",
    "print(f\"   í† í° ì ˆì•½: {inefficient_tokens - efficient_tokens} í† í°\")\n",
    "print(f\"   ì ˆì•½ë¥ : {((inefficient_tokens - efficient_tokens) / inefficient_tokens * 100):.1f}%\")\n",
    "\n",
    "# 2. ì‘ë‹µ ê¸¸ì´ ì œí•œ ë¹„êµ (ì‹œë®¬ë ˆì´ì…˜)\n",
    "print(f\"\\n2ï¸âƒ£ ì‘ë‹µ ê¸¸ì´ ì œí•œ ë¹„êµ (ì‹œë®¬ë ˆì´ì…˜)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "question = \"ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# ì œí•œ ì—†ëŠ” ëª¨ë¸ ì„¤ì •\n",
    "unlimited_llm_config = {\n",
    "    \"model\": \"gemini-1.5-flash\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_output_tokens\": None  # ì œí•œ ì—†ìŒ\n",
    "}\n",
    "\n",
    "# ì œí•œ ìˆëŠ” ëª¨ë¸ ì„¤ì •\n",
    "limited_llm_config = {\n",
    "    \"model\": \"gemini-1.5-flash\", \n",
    "    \"temperature\": 0.7,\n",
    "    \"max_output_tokens\": 100  # ì¶œë ¥ ì œí•œ\n",
    "}\n",
    "\n",
    "print(f\"ì§ˆë¬¸: {question}\")\n",
    "print(f\"ì…ë ¥ í† í°: {llm.get_num_tokens(question)}\")\n",
    "\n",
    "# API í˜¸ì¶œ ëŒ€ì‹  ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ëŒ€ì²´\n",
    "print(f\"\\nğŸ”“ ì œí•œ ì—†ëŠ” ì‘ë‹µ (ì‹œë®¬ë ˆì´ì…˜):\")\n",
    "print(f\"ì„¤ì •: {unlimited_llm_config}\")\n",
    "estimated_unlimited_output = 150  # ì˜ˆìƒ ì¶œë ¥ í† í°\n",
    "unlimited_total_estimated = llm.get_num_tokens(question) + estimated_unlimited_output\n",
    "print(f\"ì˜ˆìƒ ì¶œë ¥ í† í°: {estimated_unlimited_output}\")\n",
    "print(f\"ì˜ˆìƒ ì´ í† í°: {unlimited_total_estimated}\")\n",
    "\n",
    "print(f\"\\nğŸ”’ ê¸¸ì´ ì œí•œ ì‘ë‹µ (ì‹œë®¬ë ˆì´ì…˜):\")\n",
    "print(f\"ì„¤ì •: {limited_llm_config}\")\n",
    "estimated_limited_output = 100  # ì œí•œëœ ì¶œë ¥ í† í°\n",
    "limited_total_estimated = llm.get_num_tokens(question) + estimated_limited_output\n",
    "print(f\"ì˜ˆìƒ ì¶œë ¥ í† í°: {estimated_limited_output}\")\n",
    "print(f\"ì˜ˆìƒ ì´ í† í°: {limited_total_estimated}\")\n",
    "\n",
    "# ê²°ê³¼ ë¹„êµ\n",
    "print(f\"\\nğŸ“Š ì‘ë‹µ ê¸¸ì´ ì œí•œ íš¨ê³¼ (ì‹œë®¬ë ˆì´ì…˜):\")\n",
    "print(f\"   ì œí•œ ì—†ìŒ: {unlimited_total_estimated} í† í°\")\n",
    "print(f\"   ì œí•œ ìˆìŒ: {limited_total_estimated} í† í°\")\n",
    "print(f\"   ì ˆì•½: {unlimited_total_estimated - limited_total_estimated} í† í°\")\n",
    "\n",
    "if unlimited_total_estimated > 0:\n",
    "    savings_percent = ((unlimited_total_estimated - limited_total_estimated) / unlimited_total_estimated) * 100\n",
    "    print(f\"   ì ˆì•½ë¥ : {savings_percent:.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” max_output_tokens ì„¤ì •ìœ¼ë¡œ ì¶œë ¥ í† í°ì„ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™œìš©\n",
    "print(f\"\\n3ï¸âƒ£ ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™œìš©\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ë°˜ë³µì ì¸ ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì§ˆë¬¸ë“¤\n",
    "questions_with_context = [\n",
    "    \"ë‹¹ì‹ ì€ Python ì „ë¬¸ê°€ì…ë‹ˆë‹¤. Pythonì˜ ë¦¬ìŠ¤íŠ¸ comprehensionì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "    \"ë‹¹ì‹ ì€ Python ì „ë¬¸ê°€ì…ë‹ˆë‹¤. Pythonì˜ ë°ì½”ë ˆì´í„°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "    \"ë‹¹ì‹ ì€ Python ì „ë¬¸ê°€ì…ë‹ˆë‹¤. Pythonì˜ ì œë„ˆë ˆì´í„°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "]\n",
    "\n",
    "# ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ê°„ë‹¨í•œ ì§ˆë¬¸ë“¤\n",
    "system_message = SystemMessage(content=\"ë‹¹ì‹ ì€ Python ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\")\n",
    "optimized_questions = [\n",
    "    \"ë¦¬ìŠ¤íŠ¸ comprehensionì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "    \"ë°ì½”ë ˆì´í„°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\", \n",
    "    \"ì œë„ˆë ˆì´í„°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "]\n",
    "\n",
    "# í† í° ìˆ˜ ê³„ì‚°\n",
    "context_method_tokens = sum(llm.get_num_tokens(q) for q in questions_with_context)\n",
    "system_method_tokens = llm.get_num_tokens(system_message.content) + sum(llm.get_num_tokens(q) for q in optimized_questions)\n",
    "\n",
    "print(f\"âŒ ë°˜ë³µ ì»¨í…ìŠ¤íŠ¸ ë°©ì‹:\")\n",
    "print(f\"   ì´ ì…ë ¥ í† í°: {context_method_tokens}\")\n",
    "\n",
    "print(f\"\\nâœ… ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë°©ì‹:\")\n",
    "print(f\"   ì‹œìŠ¤í…œ ë©”ì‹œì§€: {llm.get_num_tokens(system_message.content)} í† í°\")\n",
    "print(f\"   ì§ˆë¬¸ë“¤: {sum(llm.get_num_tokens(q) for q in optimized_questions)} í† í°\")\n",
    "print(f\"   ì´ ì…ë ¥ í† í°: {system_method_tokens}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìµœì í™” ê²°ê³¼:\")\n",
    "print(f\"   í† í° ì ˆì•½: {context_method_tokens - system_method_tokens} í† í°\")\n",
    "print(f\"   ì ˆì•½ë¥ : {((context_method_tokens - system_method_tokens) / context_method_tokens * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\nâœ… í† í° ìµœì í™” ì‹¤ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e09cede",
   "metadata": {},
   "source": [
    "## ì •ë¦¬ ë° ëª¨ë²” ì‚¬ë¡€\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œ ë°°ìš´ í† í° ì‚¬ìš©ëŸ‰ í™•ì¸ ë° ê´€ë¦¬ ë°©ë²•ë“¤ì„ ì •ë¦¬í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "### ğŸ¯ ì£¼ìš” í•™ìŠµ ë‚´ìš©\n",
    "\n",
    "1. **ì‘ë‹µ ë©”íƒ€ë°ì´í„°**: `response.response_metadata`ì—ì„œ í† í° ì •ë³´ í™•ì¸\n",
    "2. **`get_num_tokens()`**: ì‚¬ì „ í† í° ê³„ì‚°ìœ¼ë¡œ ë¹„ìš© ì˜ˆì¸¡\n",
    "3. **ì½œë°± í•¸ë“¤ëŸ¬**: ì‹¤ì‹œê°„ í† í° ì¶”ì  ë° ìƒì„¸ ë¶„ì„\n",
    "4. **LangSmith**: ìë™ í† í° ëª¨ë‹ˆí„°ë§ ë° ì‹œê°í™”\n",
    "5. **ë°°ì¹˜ ì²˜ë¦¬**: ëŒ€ëŸ‰ ìš”ì²­ì˜ íš¨ìœ¨ì ì¸ í† í° ê´€ë¦¬\n",
    "6. **ìµœì í™” ê¸°ë²•**: í”„ë¡¬í”„íŠ¸, ì‘ë‹µ ê¸¸ì´, ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìµœì í™”\n",
    "\n",
    "### ğŸ“‹ í† í° ê´€ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "| ë‹¨ê³„ | í™•ì¸ ì‚¬í•­ | ë„êµ¬/ë°©ë²• |\n",
    "|------|-----------|-----------|\n",
    "| **ì‚¬ì „ ê³„íš** | ì˜ˆìƒ í† í° ìˆ˜ ê³„ì‚° | `get_num_tokens()` |\n",
    "| **ì‹¤í–‰ ì¤‘** | ì‹¤ì‹œê°„ í† í° ì¶”ì  | ì½œë°± í•¸ë“¤ëŸ¬ |\n",
    "| **ëª¨ë‹ˆí„°ë§** | ì¥ê¸°ì  ì‚¬ìš©ëŸ‰ ë¶„ì„ | LangSmith |\n",
    "| **ìµœì í™”** | í† í° íš¨ìœ¨ì„± ê°œì„  | í”„ë¡¬í”„íŠ¸ ìµœì í™” |\n",
    "\n",
    "### âš ï¸ ì£¼ì˜ì‚¬í•­\n",
    "\n",
    "1. **ì •í™•ì„±**: í† í° ê³„ì‚°ì€ ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ\n",
    "2. **ë¹„ìš© ì˜ˆì¸¡**: ì‹¤ì œ ë¹„ìš©ì€ API ì œê³µìì˜ ì •ì±…ì— ë”°ë¼ ë³€ë™\n",
    "3. **ì„±ëŠ¥ ì˜í–¥**: ê³¼ë„í•œ í† í° ì¶”ì ì€ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ\n",
    "4. **ë©”íƒ€ë°ì´í„° ì˜ì¡´ì„±**: ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "### ğŸš€ ì‹¤ì „ í™œìš© íŒ\n",
    "\n",
    "1. **ê°œë°œ ë‹¨ê³„**: ì½œë°±ì„ í™œìš©í•œ ìƒì„¸ í† í° ë¶„ì„\n",
    "2. **í…ŒìŠ¤íŠ¸ ë‹¨ê³„**: ë°°ì¹˜ ì²˜ë¦¬ë¡œ í† í° ì‚¬ìš©ëŸ‰ ê²€ì¦\n",
    "3. **í”„ë¡œë•ì…˜**: LangSmithë¡œ ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§\n",
    "4. **ìµœì í™”**: ì •ê¸°ì ì¸ í”„ë¡¬í”„íŠ¸ ë° ì„¤ì • ìµœì í™”\n",
    "\n",
    "### ğŸ’¡ ë¹„ìš© ì ˆì•½ ìš”ë ¹\n",
    "\n",
    "- **ì§§ê³  ëª…í™•í•œ í”„ë¡¬í”„íŠ¸** ì‚¬ìš©\n",
    "- **ì‘ë‹µ ê¸¸ì´ ì œí•œ** ì„¤ì • (`max_output_tokens`)\n",
    "- **ì‹œìŠ¤í…œ ë©”ì‹œì§€** í™œìš©ìœ¼ë¡œ ë°˜ë³µ ì»¨í…ìŠ¤íŠ¸ ì œê±°\n",
    "- **ìºì‹±** ë° **ë°°ì¹˜ ì²˜ë¦¬** ì ê·¹ í™œìš©\n",
    "- **ì •ê¸°ì ì¸ í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„** ë° ìµœì í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# ì¢…í•© ì˜ˆì œ: í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ\n",
    "############################################################################\n",
    "\n",
    "print(\"ğŸ¯ ì¢…í•© ì˜ˆì œ: í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class TokenMonitor:\n",
    "    \"\"\"í† í° ì‚¬ìš©ëŸ‰ì„ ì¢…í•©ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, cost_per_1k_tokens=0.001):\n",
    "        self.llm = llm\n",
    "        self.cost_per_1k_tokens = cost_per_1k_tokens\n",
    "        self.callback = TokenUsageCallback()\n",
    "        self.sessions = []\n",
    "    \n",
    "    def estimate_tokens(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì •\"\"\"\n",
    "        try:\n",
    "            return self.llm.get_num_tokens(text)\n",
    "        except:\n",
    "            return len(text) // 4  # ëŒ€ëµì ì¸ ì¶”ì •\n",
    "    \n",
    "    def process_with_monitoring(self, messages, session_name=\"ê¸°ë³¸ ì„¸ì…˜\"):\n",
    "        \"\"\"í† í° ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ë©”ì‹œì§€ ì²˜ë¦¬\"\"\"\n",
    "        print(f\"\\nğŸ”„ ì„¸ì…˜ ì‹œì‘: {session_name}\")\n",
    "        \n",
    "        # ì‚¬ì „ í† í° ì¶”ì •\n",
    "        if isinstance(messages, str):\n",
    "            messages = [HumanMessage(content=messages)]\n",
    "        elif isinstance(messages, list) and isinstance(messages[0], str):\n",
    "            messages = [HumanMessage(content=msg) for msg in messages]\n",
    "        \n",
    "        estimated_input = sum(self.estimate_tokens(msg.content) for msg in messages)\n",
    "        print(f\"   ğŸ“Š ì˜ˆìƒ ì…ë ¥ í† í°: {estimated_input}\")\n",
    "        \n",
    "        # ì‹¤ì œ ì²˜ë¦¬\n",
    "        session_callback = TokenUsageCallback()\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(\n",
    "                messages, \n",
    "                config={\"callbacks\": [session_callback]}\n",
    "            )\n",
    "            \n",
    "            # ì„¸ì…˜ ì •ë³´ ì €ì¥\n",
    "            session_info = {\n",
    "                'name': session_name,\n",
    "                'estimated_input': estimated_input,\n",
    "                'actual_tokens': session_callback.total_tokens,\n",
    "                'response': response.content,\n",
    "                'success': True\n",
    "            }\n",
    "            self.sessions.append(session_info)\n",
    "            \n",
    "            print(f\"   âœ… ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "            print(f\"   ğŸ“ˆ ì‹¤ì œ ì‚¬ìš© í† í°: {session_callback.total_tokens}\")\n",
    "            print(f\"   ğŸ’° ì„¸ì…˜ ë¹„ìš©: ${(session_callback.total_tokens/1000)*self.cost_per_1k_tokens:.6f}\")\n",
    "            \n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            session_info = {\n",
    "                'name': session_name,\n",
    "                'estimated_input': estimated_input, \n",
    "                'actual_tokens': 0,\n",
    "                'response': None,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "            self.sessions.append(session_info)\n",
    "            print(f\"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_summary_report(self):\n",
    "        \"\"\"ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        if not self.sessions:\n",
    "            return \"ì•„ì§ ì²˜ë¦¬ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        total_sessions = len(self.sessions)\n",
    "        successful_sessions = [s for s in self.sessions if s['success']]\n",
    "        failed_sessions = [s for s in self.sessions if not s['success']]\n",
    "        \n",
    "        total_tokens = sum(s['actual_tokens'] for s in successful_sessions)\n",
    "        total_cost = (total_tokens / 1000) * self.cost_per_1k_tokens\n",
    "        \n",
    "        accuracy_data = [(s['estimated_input'], s['actual_tokens']) for s in successful_sessions if s['actual_tokens'] > 0]\n",
    "        avg_accuracy = 0\n",
    "        if accuracy_data:\n",
    "            accuracies = [1 - abs(est - act) / act for est, act in accuracy_data if act > 0]\n",
    "            avg_accuracy = sum(accuracies) / len(accuracies) * 100\n",
    "        \n",
    "        report = f\"\"\"\n",
    "ğŸ“Š í† í° ì‚¬ìš©ëŸ‰ ì¢…í•© ë¦¬í¬íŠ¸\n",
    "{'='*40}\n",
    "ğŸ”¢ ì´ ì„¸ì…˜ ìˆ˜: {total_sessions}ê°œ\n",
    "   â”œâ”€ ì„±ê³µ: {len(successful_sessions)}ê°œ\n",
    "   â””â”€ ì‹¤íŒ¨: {len(failed_sessions)}ê°œ\n",
    "\n",
    "ğŸ¯ í† í° ì‚¬ìš©ëŸ‰:\n",
    "   â”œâ”€ ì´ í† í°: {total_tokens:,}ê°œ\n",
    "   â”œâ”€ í‰ê· /ì„¸ì…˜: {total_tokens/max(len(successful_sessions), 1):.1f}ê°œ\n",
    "   â””â”€ ì˜ˆì¸¡ ì •í™•ë„: {avg_accuracy:.1f}%\n",
    "\n",
    "ğŸ’° ë¹„ìš© ì •ë³´:\n",
    "   â”œâ”€ ì´ ë¹„ìš©: ${total_cost:.6f}\n",
    "   â””â”€ í‰ê· /ì„¸ì…˜: ${total_cost/max(len(successful_sessions), 1):.6f}\n",
    "\n",
    "ğŸ“‹ ì„¸ì…˜ë³„ ìƒì„¸:\"\"\"\n",
    "        \n",
    "        for i, session in enumerate(self.sessions[-5:], 1):  # ìµœê·¼ 5ê°œë§Œ\n",
    "            status = \"âœ…\" if session['success'] else \"âŒ\"\n",
    "            tokens = session['actual_tokens'] if session['success'] else 0\n",
    "            report += f\"\\n   {status} {session['name']}: {tokens} í† í°\"\n",
    "        \n",
    "        if len(self.sessions) > 5:\n",
    "            report += f\"\\n   ... (ì´ {len(self.sessions)}ê°œ ì„¸ì…˜)\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# í† í° ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "monitor = TokenMonitor(llm)\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì„¸ì…˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "test_sessions = [\n",
    "    (\"ê°„ë‹¨í•œ ì§ˆë¬¸\", \"ì•ˆë…•í•˜ì„¸ìš”!\"),\n",
    "    (\"ì¤‘ê°„ ê¸¸ì´ ì§ˆë¬¸\", \"Python í”„ë¡œê·¸ë˜ë°ì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”.\"),\n",
    "    (\"ë³µì¡í•œ ì§ˆë¬¸\", \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ê³ , ê°ê°ì˜ í™œìš© ë¶„ì•¼ì™€ ë¯¸ë˜ ì „ë§ì— ëŒ€í•´ ë…¼ì˜í•´ì£¼ì„¸ìš”.\")\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª ë‹¤ì–‘í•œ ì„¸ì…˜ìœ¼ë¡œ í† í° ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "for session_name, question in test_sessions:\n",
    "    response = monitor.process_with_monitoring(question, session_name)\n",
    "    if response:\n",
    "        print(f\"   ğŸ“ ì‘ë‹µ: {response[:50]}...\")\n",
    "\n",
    "# ì¢…í•© ë¦¬í¬íŠ¸ ì¶œë ¥\n",
    "print(monitor.get_summary_report())\n",
    "\n",
    "print(f\"\\nğŸ‰ í† í° ì‚¬ìš©ëŸ‰ í™•ì¸ ê°€ì´ë“œ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ’¡ ì´ì œ ì—¬ëŸ¬ë¶„ì€ LangChainì—ì„œ í† í°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
